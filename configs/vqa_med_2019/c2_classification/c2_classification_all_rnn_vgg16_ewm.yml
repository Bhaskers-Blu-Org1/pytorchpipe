# Load config defining problems for training, validation and testing.
default_configs: vqa_med_2019/c2_classification/default_c2_classification.yml

training:
  # settings parameters
  terminal_conditions:
    loss_stop: 1.0e-2
    episode_limit: 10000
    epoch_limit: -1


pipeline:
  name: c2_classification_all_rnn_vgg16_ewm

  global_publisher:
    priority: 0
    type: GlobalVariablePublisher
    # Add input_size to globals.
    keys: [question_encoder_output_size, image_encoder_output_size, element_wise_activation_size]
    values: [100, 100, 100]

  # First subpipeline: question.
  # Questions encoding.
  question_tokenizer:
    priority: 1.1
    type: SentenceTokenizer
    streams: 
      inputs: questions
      outputs: tokenized_questions

  # Model 1: Embeddings
  question_embeddings:
    priority: 1.2
    type: SentenceEmbeddings
    embeddings_size: 50
    pretrained_embeddings_file: glove.6B.50d.txt
    data_folder: ~/data/vqa-med
    word_mappings_file: questions.all.word.mappings.csv
    streams:
      inputs: tokenized_questions
      outputs: embedded_questions      
  
  # Model 2: RNN
  question_lstm:
    priority: 1.3
    type: RecurrentNeuralNetwork
    cell_type: LSTM
    prediction_mode: Last
    use_logsoftmax: False
    initial_state_trainable: False
    #num_layers: 5
    hidden_size: 50
    streams:
      inputs: embedded_questions
      predictions: question_activations
    globals:
      input_size: embeddings_size
      prediction_size: question_encoder_output_size

  # 3rd subpipeline: image.
  # Image encoder.
  image_encoder:
    priority: 3.1
    type: TorchVisionWrapper
    streams:
      inputs: images
      predictions: image_activations
    globals:
      prediction_size: image_encoder_output_size

  # 4th subpipeline: element wise multiplication + FF.
  question_image_fusion:
    priority: 4.1
    type: ElementWiseMultiplication
    dropout: 0.5
    streams:
      image_encodings: image_activations
      question_encodings: question_activations
      outputs: element_wise_activations
    globals:
      image_encoding_size: image_encoder_output_size
      question_encoding_size: question_encoder_output_size
      output_size: element_wise_activation_size


  classifier:
    priority: 4.2
    type: FeedForwardNetwork 
    hidden_sizes: [100]
    dropout: 0.5
    streams:
      inputs: element_wise_activations
    globals:
      input_size: element_wise_activation_size
      prediction_size: vocabulary_size_c2


  #: pipeline
