# Part of pipeline containing components constituting the "Inputs Fusion" pipeline.

# Inputs:
#  streams:
#   * tokenized_questions
#   * images
#   * image_sizes

# Outputs:
#  streams:
#   * concatenated_activations
#  globals:
#   * concatenated_activations_size

# "Inputs Fusion" 
# 0.: 

checkpoint: &checkpoint ~/image-clef-2019/experiments/c4_encoders/20190504_202441/checkpoints/glove_lstm_vgg16_att_is_cat_ffn_c123_loss_best.pt
# Loaded checkpoint: 20190504_202441
# ~/image-clef-2019/experiments/c4_encoders/20190504_202441/checkpoints/glove_lstm_vgg16_att_is_cat_ffn_c123_loss_best.pt
#  + Model 'pipe1_question_embeddings' [SentenceEmbeddings] params saved 
#  + Model 'pipe1_lstm' [RecurrentNeuralNetwork] params saved 
#  + Model 'image_encoder' [TorchVisionWrapper] params saved 
#  + Model 'image_size_encoder' [FeedForwardNetwork] params saved 
#  + Model 'question_image_fusion' [VQA_Attention] params saved 
#  + Model 'question_image_ffn' [FeedForwardNetwork] params saved 

# This one will be skipped, as this is C123 classifier!
#  + Model 'pipe6_c123_answer_classifier' [FeedForwardNetwork] params saved 

pipe_if0_hyperparameters:
  # WARNING: as we are loading the pretrained pipeline, all those values must stay!

  # Image encoder.
  image_encoder_model: &image_encoder_model vgg16
  
  # Question encoder.
  question_encoder_embeddings: &question_encoder_embeddings glove.6B.50d.txt
  # Options: '' | glove.6B.50d.txt | glove.6B.100d.txt | glove.6B.200d.txt | glove.6B.300d.txt | glove.42B.300d.txt | glove.840B.300d.txt | glove.twitter.27B.txt | mimic.fastText.no_clean.300d.pickled
  question_encoder_embeddings_size_val: &question_encoder_embeddings_size_val 50
  question_encoder_lstm_size_val: &question_encoder_lstm_size_val 50
  question_encoder_output_size_val: &question_encoder_output_size_val 100
  
  # Fusion I: image + question
  question_image_fusion_type_val: &question_image_fusion_type VQA_Attention

  # Image size encoder.
  image_size_encoder_output_size_val: &image_size_encoder_output_size_val 10

  # Fusion II: (image + question) + image size (must be = question_image_fusion_size_val + image_size_encoder_output_size_val)
  question_image_size_fusion_size_val: &question_image_size_fusion_size_val 1134


pipeline:

  ################# PIPE 0: SHARED #################

  # Add global variables.
  pipe_if0_global_publisher:
    priority: 0.11
    type: GlobalVariablePublisher
    # Add input_size to globals.
    keys: [question_encoder_output_size, image_size_encoder_input_size, image_size_encoder_output_size]
    values: [*question_encoder_output_size_val, 2, *image_size_encoder_output_size_val]

  # Statistics.
  pipe_if0_batch_size:
    priority: 0.12
    type: BatchSizeStatistics

  ################# PIPE 1: SHARED QUESTION ENCODER #################

  # Model 1: question embeddings
  pipe_if1_question_embeddings:
    priority: 1.1
    type: SentenceEmbeddings
    embeddings_size: *question_encoder_embeddings_size_val
    pretrained_embeddings_file: *question_encoder_embeddings
    data_folder: ~/data/vqa-med
    word_mappings_file: questions.all.word.mappings.csv
    # LOAD AND FREEZE #
    load: 
      file: *checkpoint
      model: pipe1_question_embeddings
    freeze: True
    ###################
    streams:
      inputs: tokenized_questions
      outputs: embedded_questions
    globals:
      embeddings_size: pipe_if1_embeddings_size     
  
  # Model 2: question RNN
  pipe_if1_lstm:
    priority: 1.2
    type: RecurrentNeuralNetwork
    cell_type: LSTM
    hidden_size: *question_encoder_lstm_size_val
    prediction_mode: Last
    initial_state: Trainable
    use_logsoftmax: False
    # LOAD AND FREEZE #
    load: 
      file: *checkpoint
      model: pipe1_lstm
    freeze: True
    ###################
    streams:
      inputs: embedded_questions
      predictions: question_activations
    globals:
      input_size: pipe_if1_embeddings_size
      prediction_size: question_encoder_output_size

  ################# PIPE 2: SHARED IMAGE ENCODER #################

  # Image encoder.
  pipe_if2_image_encoder:
    priority: 2.1
    type: TorchVisionWrapper
    model: *image_encoder_model
    return_feature_maps: True
    # LOAD AND FREEZE #
    load: 
      file: *checkpoint
      model: image_encoder
    freeze: True
    ###################
    streams:
      inputs: images
      outputs: feature_maps

  ################# PIPE 3: SHARED IMAGE SIZE ENCODER #################

  # Model - image size classifier.
  pipe_if3_image_size_encoder:
    priority: 3.1
    type: FeedForwardNetwork 
    use_logsoftmax: False
    # LOAD AND FREEZE #
    load: 
      file: *checkpoint
      model: image_size_encoder
    freeze: True
    ###################
    streams:
      inputs: image_sizes
      predictions: image_size_activations
    globals:
      input_size: image_size_encoder_input_size
      prediction_size: image_size_encoder_output_size

  ################# PIPE 4: image-question fusion  #################
  # Attention + FF.
  pipe_if4_question_image_fusion:
    priority: 4.1
    type: *question_image_fusion_type
    dropout_rate: 0.5
    # Attention params.
    latent_size: 100
    num_attention_heads: 2
    # LOAD AND FREEZE #
    load: 
      file: *checkpoint
      model: question_image_fusion
    freeze: True
    ###################
    streams:
      image_encodings: feature_maps
      question_encodings: question_activations
      outputs: fused_activations
    globals:
      question_encoding_size: question_encoder_output_size
      output_size: fused_activation_size


  pipe_if4_question_image_ffn:
    priority: 4.2
    type: FeedForwardNetwork 
    dropout_rate: 0.5
    use_logsoftmax: False
    # LOAD AND FREEZE #
    load: 
      file: *checkpoint
      model: question_image_ffn
    freeze: True
    ###################
    streams:
      inputs: fused_activations
      predictions: question_image_activations
    globals:
      input_size: fused_activation_size
      prediction_size: fused_activation_size

  ################# PIPE 5: image-question-image size fusion #################

  # 5th subpipeline: concatenation 
  pipe_if5_concat:
    priority: 5.1
    type: Concatenation
    input_streams: [question_image_activations,image_size_activations]
    # Concatenation 
    dim: 1 # default
    input_dims: [[-1,1124],[-1,*image_size_encoder_output_size_val]]
    output_dims: [-1,*question_image_size_fusion_size_val]
    streams:
      outputs: concatenated_activations
    globals:
      output_size: concatenated_activations_size
