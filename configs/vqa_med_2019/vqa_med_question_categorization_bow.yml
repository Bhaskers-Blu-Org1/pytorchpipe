# Training parameters:
training:
  problem:
    type: &p_type VQAMED2019
    data_folder: &data_folder ~/data/vqa-med
    split: training
    resize_image: &resize_image [224, 224]
    batch_size:  64

  # optimizer parameters:
  optimizer:
    name: Adam
    lr: 0.1

  # settings parameters
  terminal_conditions:
    loss_stop: 1.0e-2
    episode_limit: 10000
    epoch_limit: 100

# Validation parameters:
validation:
  partial_validation_interval: 100
  problem:
    type: *p_type
    data_folder: *data_folder
    split: validation
    resize_image: *resize_image     
    batch_size:  64

# Testing parameters:
testing:
  problem:
    type: *p_type 
    data_folder: *data_folder
    split: test
    resize_image: *resize_image     
    batch_size: 64

pipeline:
  name: vqa_med_question_categorization_bow
  #load: /users/tomaszkornuta/experiments/dummylanguageidentification/language_classifier/20190301_145416/checkpoints/language_classifier_best.pt
  #freeze: True
  #disable: prediction_decoder,accuracy

  # Questions encoding.
  question_tokenizer:
    type: SentenceTokenizer
    priority: 1.1
    streams: 
      inputs: questions
      outputs: tokenized_questions

  sentence_encoder:
    type: SentenceEncoder
    priority: 1.2
    data_folder: *data_folder
    word_mappings_file: question.all.vocabulary.mappings.csv
    streams:
      inputs: tokenized_questions
      outputs: encoded_questions
    globals:
      vocabulary_size: sentence_vocabulary_size

  bow_encoder:
    type: BOWEncoder
    priority: 1.3
    streams:
      inputs: encoded_questions
      outputs: bow_questions
    globals:
        bow_size: sentence_vocabulary_size # Set by sentence_encoder.

  # Model
  classifier:
    type: SoftmaxClassifier
    #freeze: True
    priority: 3
    streams:
      inputs: bow_questions
    globals:
      input_size: sentence_vocabulary_size # Set by sentence_encoder.
      prediction_size: num_categories # C1,C2,C3,C4

  # Loss
  nllloss:
    type: NLLLoss
    priority: 6
    targets_dim: 1
    streams:
      targets: categories
      loss: loss

  # Statistics.
  accuracy:
    type: Accuracy
    priority: 10
    streams:
      targets: categories

  #: pipeline
