# Load config defining tasks for training, validation and testing.
default_configs: vqa_med_2019/default_vqa_med_2019.yml

# Training parameters:
training:
  task:
    categories: C1,C2,C3
    export_sample_weights: ~/data/vqa-med/answers.c1_c2_c3_binary_yn.weights.csv
    # Appy all preprocessing/data augmentations.
    question_preprocessing: lowercase,remove_punctuation,tokenize
    streams: 
      questions: tokenized_questions
  sampler:
    weights: ~/data/vqa-med/answers.c1_c2_c3_binary_yn.weights.csv

# Validation parameters:
validation:
  task:
    categories: C1,C2,C3
    # Appy all preprocessing/data augmentations.
    question_preprocessing: lowercase,remove_punctuation,tokenize
    streams: 
      questions: tokenized_questions


pipeline:
  
  ################# PIPE 0: SHARED #################

  # Add global variables.
  global_publisher:
    priority: 0
    type: GlobalVariablePublisher
    # Add input_size to globals.
    keys: [question_encoder_output_size, image_size_encoder_input_size, image_size_encoder_output_size, image_encoder_output_size, element_wise_activation_size, category_c123_without_yn_word_to_ix]
    values: [100, 2, 10, 100, 100, {"C1": 0, "C2": 1, "C3": 2}]

  # Statistics.
  batch_size:
    priority: 0.1
    type: BatchSizeStatistics

  ################# PIPE 0: CATEGORY #################

  # Model 1: question embeddings
  pipe0_question_embeddings:
    priority: 0.3
    type: SentenceEmbeddings
    # LOAD AND FREEZE #
    load: 
      file: ~/image-clef-2019/experiments/q_categorization/20190416_120801/checkpoints/vqa_med_question_categorization_rnn_ffn_best.pt
      model: question_embeddings
    freeze: True
    ###################
    embeddings_size: 50
    pretrained_embeddings_file: glove.6B.50d.txt
    data_folder: ~/data/vqa-med
    word_mappings_file: questions.all.word.mappings.csv
    streams:
      inputs: tokenized_questions
      outputs: pipe0_embedded_questions      
  
  # Model 2: question RNN
  pipe0_lstm:
    priority: 0.4
    type: RecurrentNeuralNetwork
    cell_type: LSTM
    # LOAD AND FREEZE #
    load: 
      file: ~/image-clef-2019/experiments/q_categorization/20190416_120801/checkpoints/vqa_med_question_categorization_rnn_ffn_best.pt
      model: lstm
    freeze: True
    ###################
    prediction_mode: Last
    initial_state: Trainable
    use_logsoftmax: False
    streams:
      inputs: pipe0_embedded_questions
      predictions: pipe0_question_activations
    globals:
      input_size: embeddings_size
      prediction_size: question_encoder_output_size

  # Model 3: FFN question category
  pipe0_classifier:
    priority: 0.5
    type: FeedForwardNetwork
    # LOAD AND FREEZE #
    load: 
      file: ~/image-clef-2019/experiments/q_categorization/20190416_120801/checkpoints/vqa_med_question_categorization_rnn_ffn_best.pt
      model: classifier
    freeze: True
    ###################
    hidden: [50]
    dropout_rate: 0.5
    streams:
      inputs: pipe0_question_activations
      predictions: pipe0_predicted_question_categories_preds
    globals:
      input_size: question_encoder_output_size # Set by global publisher
      prediction_size: num_categories # C1,C2,C3,C4, BINARY, UNK

  pipe0_category_decoder:
    priority: 0.6
    type: WordDecoder
    # Use the same word mappings as label indexer.
    import_word_mappings_from_globals: True
    streams:
      inputs: pipe0_predicted_question_categories_preds
      outputs: pipe0_predicted_question_categories_names
    globals:
      vocabulary_size: num_categories
      word_mappings: category_word_mappings

  pipe0_category_accuracy:
    priority: 0.7
    type: AccuracyStatistics
    streams:
      targets: category_ids
      predictions: pipe0_predicted_question_categories_preds
    statistics:
      accuracy: categorization_accuracy
  
  ################# PIPE 1: SHARED QUESTION ENCODER #################

  # Model 1: question embeddings
  pipe1_question_embeddings:
    priority: 1.1
    type: SentenceEmbeddings
    embeddings_size: 50
    pretrained_embeddings_file: glove.6B.50d.txt
    data_folder: ~/data/vqa-med
    word_mappings_file: questions.all.word.mappings.csv
    streams:
      inputs: tokenized_questions
      outputs: embedded_questions      
  
  # Model 2: question RNN
  pipe1_lstm:
    priority: 1.2
    type: RecurrentNeuralNetwork
    cell_type: LSTM
    prediction_mode: Last
    initial_state: Trainable
    use_logsoftmax: False
    streams:
      inputs: embedded_questions
      predictions: question_activations
    globals:
      input_size: embeddings_size
      prediction_size: question_encoder_output_size

  # Answer encoding
  pipe1_all_answer_indexer:
    priority: 1.3
    type: LabelIndexer
    data_folder: ~/data/vqa-med
    word_mappings_file: answers.c1_c2_c3_without_yn.word.mappings.csv
    # Export mappings and size to globals.
    export_word_mappings_to_globals: True
    streams:
      inputs: answers
      outputs: all_answers_ids
    globals:
      vocabulary_size: vocabulary_size_c123_without_yn
      word_mappings: word_mappings_c123_without_yn

  ################# PIPE 2: SHARED IMAGE ENCODER #################

  # Image encoder.
  image_encoder:
    priority: 2.1
    type: TorchVisionWrapper
    model: resnet50
    streams:
      inputs: images
      outputs: image_activations
    globals:
      output_size: image_encoder_output_size

  ################# PIPE 3: SHARED IMAGE SIZE ENCODER #################

  # Model - image size classifier.
  image_size_encoder:
    priority: 3.1
    type: FeedForwardNetwork 
    use_losfotmax: False
    streams:
      inputs: image_sizes
      predictions: image_size_activations
    globals:
      input_size: image_size_encoder_input_size
      prediction_size: image_size_encoder_output_size

  ################# PIPE 4: image-question fusion  #################
  # Element wise multiplication + FF.
  question_image_fusion:
    priority: 4.1
    type: ElementWiseMultiplication
    dropout_rate: 0.5
    streams:
      image_encodings: image_activations
      question_encodings: question_activations
      outputs: element_wise_activations
    globals:
      image_encoding_size: image_encoder_output_size
      question_encoding_size: question_encoder_output_size
      output_size: element_wise_activation_size

  question_image_ffn:
    priority: 4.2
    type: FeedForwardNetwork 
    hidden_sizes: [100]
    dropout_rate: 0.5
    use_logsoftmax: False
    streams:
      inputs: element_wise_activations
      predictions: question_image_activations
    globals:
      input_size: element_wise_activation_size
      prediction_size: element_wise_activation_size

  ################# PIPE 5: image-question-image size fusion #################

  # 5th subpipeline: concatenation 
  concat:
    priority: 5.1
    type: Concatenation
    input_streams: [question_image_activations,image_size_activations]
    # Concatenation 
    dim: 1 # default
    input_dims: [[-1,100],[-1,10]]
    output_dims: [-1,110]
    streams:
      outputs: concatenated_activations
    globals:
      output_size: concatenated_activations_size

  ################# PIPE 6: C1 + C2 + C3 questions #################

  # Answer encoding for PIPE 6.
  pipe6_c123_answer_indexer:
    priority: 6.1
    type: LabelIndexer
    data_folder: ~/data/vqa-med
    word_mappings_file: answers.c1_c2_c3_without_yn.word.mappings.csv
    # Export mappings and size to globals.
    export_word_mappings_to_globals: True
    streams:
      inputs: answers
      outputs: pipe6_c123_answers_ids
    globals:
      vocabulary_size: vocabulary_size_c123_without_yn
      word_mappings: word_mappings_c123_without_yn

  # Sample masking based on categories.
  pipe6_c123_string_to_mask:
    priority: 6.2
    type: StringToMask
    globals:
      word_mappings: category_c123_without_yn_word_to_ix
    streams:
      strings: pipe0_predicted_question_categories_names
      string_indices: predicted_c123_by_question_categories_indices # NOT USED
      masks: pipe6_c123_masks

  # Model 4: FFN C123 answering
  pipe6_c123_answer_classifier:
    priority: 6.3
    type: FeedForwardNetwork
    hidden: [100]
    dropout_rate: 0.5
    streams:
      inputs: concatenated_activations
      predictions: pipe6_c123_predictions
    globals:
      input_size: concatenated_activations_size
      prediction_size: vocabulary_size_c123_without_yn

  pipe6_c123_nllloss:
    priority: 6.4
    type: NLLLoss
    targets_dim: 1
    use_masking: True
    streams:
      predictions: pipe6_c123_predictions
      masks: pipe6_c123_masks
      targets: pipe6_c123_answers_ids
      loss: pipe6_c123_loss

  pipe6_c123_precision_recall:
    priority: 6.5
    type: PrecisionRecallStatistics
    use_word_mappings: True
    use_masking: True
    show_class_scores: True
    #show_confusion_matrix: True
    streams:
      masks: pipe6_c123_masks
      predictions: pipe6_c123_predictions
      targets: pipe6_c123_answers_ids
    globals:
      word_mappings: word_mappings_c123_without_yn
    statistics:
      precision: pipe6_c123_precision
      recall: pipe6_c123_recall
      f1score: pipe6_c123_f1score

  # C123 Predictions decoder.
  pipe5_c123_prediction_decoder:
    priority: 6.6
    type: WordDecoder
    # Use the same word mappings as label indexer.
    import_word_mappings_from_globals: True
    streams:
      inputs: pipe6_c123_predictions
      outputs: predicted_answers
    globals:
      word_mappings: word_mappings_c123_without_yn

  ################# PIPE 9: MERGE ANSWERS #################

  # Viewers.
  viewer:
    priority: 9.3
    type: StreamViewer
    input_streams:
      tokenized_questions, category_names,
      pipe0_predicted_question_categories_names,
      pipe6_c123_masks,
      answers, predicted_answers


#: pipeline
