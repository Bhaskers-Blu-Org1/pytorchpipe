# Training parameters:
training:
  problem: 
    type: MNIST
    batch_size: &b 64
    use_train_data: True
    #resize: [32, 32]
  # Use sampler that operates on a subset.
  sampler:
    type: SubsetRandomSampler
    indices: [0, 55000]
  # optimizer parameters:
  optimizer:
    type: Adam
    lr: 0.0001
  # settings parameters
  terminal_conditions:
    loss_stop_threshold: 0.05
    early_stop_validations: 10
    episode_limit: 10000
    epoch_limit: 10

# Validation parameters:
validation:
  #partial_validation_interval: 100
  problem:
    type: MNIST
    batch_size: *b
    use_train_data: True  # True because we are splitting the training set to: validation and training
    #resize: [32, 32]
  # Use sampler that operates on a subset.
  sampler:
    type: SubsetRandomSampler
    indices: [55000, 60000]

# Testing parameters:
test:
  problem:
    type: MNIST
    batch_size: *b
    use_train_data: False
    #resize: [32, 32]

pipeline:

  # Loss
  nllloss:
    type: NLLLoss
    priority: 10.0

  # Statistics.
  batch_size:
    type: BatchSizeStatistics
    priority: 100.0

  accuracy:
    type: AccuracyStatistics
    priority: 100.1


  precision_recall:
    type: PrecisionRecallStatistics
    priority: 100.2
    use_word_mappings: True
    show_class_scores: True
    globals:
      word_mappings: label_word_mappings
        

