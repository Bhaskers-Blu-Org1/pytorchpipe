# Training parameters:
training:
  task: 
    type: MNIST
    batch_size: &b 64
    use_train_data: True
    #resize: [32, 32]
  # Use sampler that operates on a subset.
  sampler:
    type: SubsetRandomSampler
    indices: [0, 55000]
  # optimizer parameters:
  optimizer:
    type: Adam
    lr: 0.0001
  # settings parameters
  terminal_conditions:
    loss_stop_threshold: 0.05
    early_stop_validations: -1
    episode_limit: 10000
    epoch_limit: 10

# Validation parameters:
validation:
  #partial_validation_interval: 100
  task:
    type: MNIST
    batch_size: *b
    use_train_data: True  # True because we are splitting the training set to: validation and training
    #resize: [32, 32]
  # Use sampler that operates on a subset.
  sampler:
    type: SubsetRandomSampler
    indices: [55000, 60000]

# Testing parameters:
test:
  task:
    type: MNIST
    batch_size: *b
    use_train_data: False
    #resize: [32, 32]

pipeline:
  disable: image_viewer

  # Loss
  nllloss:
    type: NLLLoss
    priority: 10.0

  # Statistics.
  batch_size:
    priority: 100.0
    type: BatchSizeStatistics

  accuracy:
    priority: 100.1
    type: AccuracyStatistics


  precision_recall:
    priority: 100.2
    type: PrecisionRecallStatistics
    use_word_mappings: True
    show_class_scores: True
    globals:
      word_mappings: label_word_mappings

  answer_decoder:
    priority: 100.3
    type: WordDecoder
    import_word_mappings_from_globals: True
    globals:
      word_mappings: label_word_mappings
    streams:
      inputs: predictions
      outputs: answers

  stream_viewer:
    priority: 100.4
    type: StreamViewer
    input_streams: labels, answers

  image_viewer:
    priority: 100.5
    type: ImageViewer
    streams:
      images: inputs
      labels: labels
      answers: answers

