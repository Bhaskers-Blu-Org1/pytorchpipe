# Training parameters:
training:
  problem: 
    type: MNIST
    batch_size: &b 64
    use_train_data: True
    data_folder: &folder '~/data/mnist'
  # Use sampler that operates on a subset.
  sampler:
    name: SubsetRandomSampler
    indices: [0, 55000]
  # optimizer parameters:
  optimizer:
    name: Adam
    lr: 0.01
  # settings parameters
  terminal_conditions:
    loss_stop: 1.0e-2
    episode_limit: 10000
    epoch_limit: 10

# Validation parameters:
validation:
  partial_validation_interval: 500
  problem:
    type: MNIST
    batch_size: *b
    use_train_data: True  # True because we are splitting the training set to: validation and training
    data_folder: *folder
  # Use sampler that operates on a subset.
  sampler:
    name: SubsetRandomSampler
    indices: [55000, 60000]

# Testing parameters:
testing:
  problem:
    type: MNIST
    batch_size: *b
    use_train_data: False
    data_folder: *folder


pipeline:
  name: mnist_softmax_classifier
  #disable: nllloss
  # Reshape inputs
  reshape:
    type: ReshapeTensor
    input_dims: [-1, 1, 28, 28]
    output_dims: [-1, 784]
    priority: 1
    keymappings:
      outputs: reshaped_images
      output_size: reshaped_image_size

  # Image classifier.
  classifier:
    type: SoftmaxClassifier 
    priority: 2
    keymappings:
      inputs: reshaped_images
      input_size: reshaped_image_size
      prediction_size: num_classes

  # Loss
  nllloss:
    type: NLLLoss
    priority: 3
  
  # Statistics.
  batch_size:
    type: BatchSize
    priority: 4
  accuracy:
    type: Accuracy
    priority: 5


#: pipeline
