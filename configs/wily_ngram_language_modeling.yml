  # Training parameters:
training:
  problem:
    type: &p_type WiLYNGramLanguageModeling
    data_folder: &data_folder '~/data/language_identification/tiny-wily'
    context: &context 2
    batch_size:  64
    use_train_data: True
    keymappings: &p_keymappings
      inputs: ngrams
      #targets: targets

  # Use sampler that operates on a subset.
  #sampler:
  #  name: SubsetRandomSampler
  #  indices: [0, 117000]

  # optimizer parameters:
  optimizer:
    name: Adam
    lr: 0.1

  # settings parameters
  terminal_conditions:
    loss_stop: 1.0e-2
    episode_limit: 10000
    epoch_limit: 100

# Validation parameters:
validation:
  partial_validation_interval: 10
  problem:
    type: *p_type
    data_folder: *data_folder
    context: *context
    batch_size:  64
    use_train_data: True
    keymappings: *p_keymappings

  # Use sampler that operates on a subset.
  #sampler:
  #  name: SubsetRandomSampler
  #  indices: [117000, 117500]

# Testing parameters:
testing:
  problem:
    type: *p_type 
    data_folder: *data_folder
    context: *context
    batch_size: 64
    use_train_data: False
    keymappings: *p_keymappings

pipeline:
  name: ngram_language_modeling
  #load: /users/tomaszkornuta/experiments/dummylanguageidentification/language_classifier/20190301_145416/checkpoints/language_classifier_best.pt
  #freeze: True
  #disable: prediction_decoder,accuracy

  # Source encoding.
  sentence_tokenizer:
    type: SentenceTokenizer
    priority: 1
    keymappings: 
      inputs: ngrams
      outputs: tokenized_ngrams

  source_indexer:
    type: SentenceIndexer
    priority: 2
    data_folder: *data_folder
    source_files: x_train.txt,x_test.txt
    encodings_file: word_encodings.csv
    keymappings:
      inputs: tokenized_ngrams
      outputs: indexed_ngrams
  
  # Model 1: word embeddings.
  sentence_embedder:
    type: Embeddings
    priority: 3
    embeddings_size: 300 # 235 * 4
    keymappings:
      inputs: indexed_ngrams
      outputs: embedded_ngrams
      vocab_size: sentence_vocab_size
      

  # Targets encoding - use the same word encodings as for sentences.
  target_indexer:
    type: LabelIndexer
    priority: 4
    data_folder: *data_folder
    source_files: x_train.txt,x_test.txt
    encodings_file: word_encodings.csv
    keymappings:
      inputs: targets
      outputs: indexed_targets
  
  # Reshapes tensors.
  reshaper:
    type: ReshapeTensor
    input_dims: [-1, -1, 300]
    output_dims: [-1, 600]
    priority: 5
    keymappings:
      inputs: embedded_ngrams
      outputs: reshaped_ngrams
      output_size: reshaped_ngrams_size

  # Model 2: classifier.
  classifier:
    type: SoftmaxClassifier 
    priority: 6
    hidden_sizes: [500]
    keymappings:
      inputs: reshaped_ngrams
      input_size: reshaped_ngrams_size
      prediction_size: sentence_vocab_size

  # Loss.
  nllloss:
    type: NLLLoss
    priority: 7
    keymappings:
      targets: indexed_targets
      predictions: predictions
      loss: loss

  # Predictions decoder.
  prediction_decoder:
    type: WordDecoder
    priority: 8
    data_folder: *data_folder
    encodings_file: word_encodings.csv
    keymappings:
      inputs: predictions
      outputs: predicted_labels

  # Statistics.
  accuracy:
    type: Accuracy
    priority: 10
    keymappings:
      targets: indexed_targets


  #: pipeline
