# Load config defining CLEVR tasks for training, validation and testing.
default_configs: clevr/default_clevr.yml

# Resize and normalize images - in all sets.
training:
  task: 
    resize_image: [224, 224]
    image_preprocessing: normalize

validation:
  task: 
    resize_image: [224, 224]
    image_preprocessing: normalize

test:
  task: 
    resize_image: [224, 224]
    image_preprocessing: normalize

# Definition of the pipeline.
pipeline:

  global_publisher:
    priority: 0
    type: GlobalVariablePublisher
    keys: [question_encoder_output_size, image_encoder_output_size]
    values: [100, 100]

  ##################################################################
  # 1st pipeline: question.
  # Questions encoding.
  question_tokenizer:
    priority: 1.1
    type: SentenceTokenizer
    # Lowercase all letters + remove punctuation (reduced vocabulary of 80 words instead of 87)
    preprocessing: all
    streams: 
      inputs: questions
      outputs: tokenized_questions

  # Model 1: Embeddings
  question_embeddings:
    priority: 1.2
    type: SentenceEmbeddings
    embeddings_size: 50
    pretrained_embeddings_file: glove.6B.50d.txt
    data_folder: ~/data/CLEVR_v1.0
    word_mappings_file: questions.all.word.mappings.lowercase.csv
    export_word_mappings_to_globals: True
    globals:
      word_mappings: question_word_mappings
      vocabulary_size: num_question_words
    streams:
      inputs: tokenized_questions
      outputs: embedded_questions      
  
  # Model 2: RNN
  lstm:
    priority: 1.3
    type: RecurrentNeuralNetwork
    cell_type: LSTM
    prediction_mode: Last
    initial_state: Zero
    hidden_size: 50
    # Turn of softmax.
    use_logsoftmax: False
    streams:
      inputs: embedded_questions
      predictions: question_activations
    globals:
      input_size: embeddings_size
      prediction_size: question_encoder_output_size

  ##################################################################
  # 2nd subpipeline: image.
  # Image encoder.
  image_encoder:
    priority: 2.1
    type: TorchVisionWrapper
    model_type: vgg16
    streams:
      inputs: images
      outputs: image_activations
    globals:
      output_size: image_encoder_output_size

  ##################################################################
  # 3rd subpipeline: concatenation + FF.
  concat:
    type: Concatenation
    priority: 3.1
    input_streams: [question_activations,image_activations]
    dim: 1 # default
    input_dims: [[-1,100],[-1,100]]
    output_dims: [-1,200]
    streams:
      outputs: concatenated_activations
    globals:
      output_size: concatenated_size

  classifier:
    type: FeedForwardNetwork 
    hidden_sizes: [100]
    priority: 3.2
    streams:
      inputs: concatenated_activations
    globals:
      input_size: concatenated_size
      prediction_size: num_answers

#: pipeline
