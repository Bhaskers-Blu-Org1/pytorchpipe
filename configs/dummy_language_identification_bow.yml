# Training parameters:
training:
  problem: &problem
    type: DummyLanguageIdentification
    data_folder: ~/data/language_identification/dummy
    batch_size:  2
    use_train_data: True
    keymappings:
      inputs: sentences
      targets: languages

  # optimizer parameters:
  optimizer:
    name: SGD
    lr: 0.1

  # settings parameters
  terminal_conditions:
    loss_stop: 1.0e-2
    episode_limit: 10000
    epoch_limit: 100

# Validation parameters:
validation:
  problem: *problem
  partial_validation_interval: 10

# Testing parameters:
testing:
  problem: *problem

pipeline:
  name: language_classifier
  load: /users/tomaszkornuta/experiments/dummylanguageidentification/language_classifier/20190301_145416/checkpoints/language_classifier_best.pt
  #freeze: True
  #disable: prediction_decoder,accuracy
  # Sentences encoding.
  sentence_tokenizer:
      type: SentenceTokenizer
      priority: 1
      keymappings: 
        inputs: sentences
        outputs: tokenized_sentences
  sentence_encoder:
      type: SentenceEncoder
      priority: 2
      data_folder: ~/data/language_identification/dummy
      source_files: x_training.txt,x_test.txt
      encodings_file: word_encodings.csv
      keymappings:
        inputs: tokenized_sentences
        outputs: encoded_sentences
  bow_encoder:
      type: BOWEncoder
      priority: 3
      keymappings:
        inputs: encoded_sentences
        outputs: bow_sencentes
        input_size: sentence_token_size # Set by sentence_encoder.
  
  # Targets encoding.
  label_encoder:
      type: LabelEncoder
      priority: 4
      data_folder: ~/data/language_identification/dummy
      source_files: y_training.tx,ty_test.txt
      encodings_file: language_name_encodings.csv
      keymappings:
        inputs: languages
        outputs: encoded_languages
  
  # Model
  classifier:
    type: SoftmaxClassifier
    #freeze: True
    priority: 5
    keymappings:
      inputs: bow_sencentes
      #predictions: encoded_predictions
      input_size: sentence_token_size # Set by sentence_encoder.
      prediction_size: label_token_size # Set by target_encoder.
  
  # Loss
  nllloss:
    type: NLLLoss
    priority: 6
    keymappings:
      targets: encoded_languages
      #predictions: encoded_predictions
      loss: loss
  # Predictions decoder.
  prediction_decoder:
    type: WordDecoder
    priority: 8
    data_folder: ~/data/language_identification/dummy
    encodings_file: language_name_encodings.csv
    keymappings:
      inputs: predictions
      outputs: predicted_labels

  # Statistics.
  batch_size:
    type: BatchSize
    priority: 9
  accuracy:
    type: Accuracy
    priority: 10
    keymappings:
      targets: encoded_languages

  #: pipeline
